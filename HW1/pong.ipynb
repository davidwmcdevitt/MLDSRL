{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328904d9-d179-4889-a046-84594544218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display as ipy_display\n",
    "from pyvirtualdisplay import Display\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46358a48-60b9-4367-bece-53598c3d8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2f9eb-16d4-40ef-a014-ee142bd029a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return np.reshape(image.astype(float).ravel(), [80,80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19995c04-1e6e-4138-a83c-6e9389e0c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee50536-9257-4796-ab72-5653b08dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb9e2b-8df4-4007-9089-10db9a518939",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = torch.tensor(preprocess(state)).unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80edd6c7-0717-48f9-a8d4-f3fd2f386cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(10, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 10 * 10, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Apply He initialization\n",
    "        init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.conv3.weight, nonlinearity='relu')\n",
    "        \n",
    "        init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = x.view(-1, 128 * 10 * 10)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bddce-ef0d-4557-b16e-e3089d7256e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "policy_net = PolicyNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db6e23-3b8c-4293-b32c-48839fa754f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a27cea-894f-438e-980a-e49ccd0a8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = torch.zeros((4, 80, 80))\n",
    "window.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76934305-46c8-4769-a1a1-c2fd18674779",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = torch.cat((screen, window), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6854911-0547-4258-9f7a-0f4b8f80e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = policy_net(window.unsqueeze(0).to(device))\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output probabilities:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298b39d-f2f6-410b-9d43-2c980acf07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(policy_net, state, window, prior, method):\n",
    "  if method == \"random\":\n",
    "    action = np.random.choice(2) + 2\n",
    "    probs = np.array([[0.5, 0.5]])\n",
    "  if method == \"gradient\":\n",
    "    screen = torch.tensor(preprocess(state)).unsqueeze(0).float()\n",
    "    #screen = torch.where(screen != 0, torch.tensor(0.0), torch.tensor(1.0))\n",
    "    screen = screen - prior\n",
    "    screen = screen\n",
    "    window = torch.cat((screen, window), dim=0)\n",
    "    input_frame = window\n",
    "    input_frame = input_frame.unsqueeze(0).to(device)\n",
    "    probs = policy_net(input_frame)\n",
    "    window = window[:9,:,:]\n",
    "    prior = screen\n",
    "    \n",
    "    try:\n",
    "        probs = probs.cpu().detach().numpy()\n",
    "        action = np.random.choice(2, p=probs) + 2\n",
    "    except ValueError:\n",
    "        action = np.random.choice(2) + 2\n",
    "\n",
    "  return action, probs, window, prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d9c11-2d75-46f5-b736-320bac299747",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=0.001)\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1017e3-2745-4a31-82bd-1381a2548695",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_episode = 0\n",
    "longest_episode_length = 0\n",
    "longest_episode_path = \"\"\n",
    "episode_lengths = []\n",
    "loss_list = []\n",
    "rolling_rewards = []\n",
    "rolling_rewards_avg = []\n",
    "rolling_time_avg = []\n",
    "\n",
    "episode =  0\n",
    "count = 0\n",
    "cooldown = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e63e4a-3d77-48b2-919f-e003d12ec1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(10000):\n",
    "    \n",
    "    state, info = env.reset()\n",
    "    t = 0\n",
    "    \n",
    "    obs_history = []\n",
    "    action_history = []\n",
    "    episode_rewards = []\n",
    "    saved_log_probs = []\n",
    "    \n",
    "    terminated, truncated = False, False\n",
    "    \n",
    "    window = torch.zeros((9, 80, 80))\n",
    "    prior = torch.zeros((1, 80, 80))\n",
    "    \n",
    "    while not terminated:\n",
    "        t+=1\n",
    "        if (episode) % 25 == 0 | episode <= 25:\n",
    "            action, probs, window, prior = get_action(policy_net, state, window, prior, \"random\")\n",
    "        else:\n",
    "            action, probs, window, prior = get_action(policy_net, state, window, prior, \"gradient\")\n",
    "            \n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        obs_history.append(observation)\n",
    "        episode_rewards.append(reward)\n",
    "        action_history.append(action)\n",
    "        saved_log_probs.append(1 + np.log(probs)[0][action-2])\n",
    "        #print(torch.log(policy_net(torch.tensor(preprocess(state)).unsqueeze(0).float().to(device)))[0][action-2])\n",
    "        \n",
    "    #print(torch.log(policy_net(torch.tensor(preprocess(state)).unsqueeze(0).float().to(device)))[0][action-2])    \n",
    "    #print(f\"Episode {episode+1} finished after {t} time steps.\")\n",
    "    \n",
    "    rolling_rewards.append(sum(episode_rewards))\n",
    "    rolling_rewards_avg.append(np.mean(rolling_rewards[-100:]))\n",
    "    discounted_rewards = []\n",
    "    R = 0\n",
    "    for i, r in enumerate(reversed(episode_rewards)):\n",
    "        r = 10 * r + 1\n",
    "        if episode_rewards[i] != 0: R = 0\n",
    "        R = r + gamma * R\n",
    "        discounted_rewards.insert(0, R)\n",
    "\n",
    "    discounted_rewards = torch.tensor(discounted_rewards, requires_grad = True)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "\n",
    "    #loss = -torch.mean(torch.mul(torch.stack(saved_log_probs), discounted_rewards.to(device)))\n",
    "    loss = -torch.sum(discounted_rewards).to(device)\n",
    "    loss_list.append(loss.cpu().detach().numpy())\n",
    "    #print(f\"Episode {episode + 1} loss: {loss}\")\n",
    "    episode_lengths.append(t)\n",
    "    rolling_time_avg.append(np.mean(episode_lengths[-100:]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(\"-\"*50)\n",
    "        print(f\"Episode {episode+1} finished after {t} time steps.\")\n",
    "        print(f\"Episode {episode+1} average time per 100: {np.mean(episode_lengths[-100:])} time steps.\")\n",
    "        print(f\"Episode {episode + 1} loss: {loss}\")\n",
    "        print(f\"Episode {episode + 1} discounted rewards: {torch.mean(discounted_rewards)}\")\n",
    "        print(f\"Episode {episode + 1} saved log probs: {np.mean(saved_log_probs)}\")\n",
    "        print(f\"Episode {episode + 1} Action Means: {np.mean(action_history)}\")\n",
    "        print(f\"Episode {episode + 1} Reward Rolling Average per 100: {np.mean(rolling_rewards[-100:])}\")\n",
    "        print(\"-\"*50)\n",
    "    \n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d35fd-28c5-4f0e-9ba0-511dc0797eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episode_lengths, marker='o')\n",
    "\n",
    "plt.title('Episode Lengths Over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('results/pong_duration.png', dpi=300, format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79b8dd-6ecd-4558-8a32-636b80e7fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_list, marker='o')\n",
    "\n",
    "plt.title('Loss Over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('results/pong_loss.png', dpi=300, format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9237451-48b0-4f48-a410-98f079f46c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rolling_rewards_avg, marker='o')\n",
    "\n",
    "plt.title('Avg Reward Over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward (per 100)')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('results/pong_avg_reward.png', dpi=300, format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2615f-a767-4d23-a989-ace675c6a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rolling_time_avg, marker='o')\n",
    "\n",
    "plt.title('Avg Lifespan Over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Lifespan (per 100)')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('results/pong_avg_life.png', dpi=300, format='png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldsrl",
   "language": "python",
   "name": "mldsrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
