# -*- coding: utf-8 -*-
"""AutoML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17wGDvdV1qylk7_xjXSgSb9IintYcDfvK
"""

import numpy as np
import torch
from torch import nn, optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split, TensorDataset
import torch.nn.functional as F
import matplotlib.pyplot as plt
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import copy

#!unzip -o '/content/Assignment4data.zip'

X_train = np.load('Assignment4data/train_X.npy', allow_pickle=True)
y_train = np.load('Assignment4data/train_y.npy', allow_pickle=True)
X_test = np.load('Assignment4data/test_X.npy', allow_pickle=True)
y_test = np.load('Assignment4data/test_y.npy', allow_pickle=True)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32).view(-1, 28*28)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32).view(-1, 28*28)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32).view(-1, 28*28)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
test_dataset = TensorDataset(X_val_tensor, y_val_tensor)

batch_sizes = list(range(16, 1025))
#batch_sizes = [1,4]
activation_functions = ['relu', 'sigmoid', 'tanh']

class NeuralNet(nn.Module):
    def __init__(self, input_size, num_classes, genome):
        super(NeuralNet, self).__init__()

        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, num_classes)

        if genome['activation_function'] == 'relu':
            self.activation = nn.ReLU()
        elif genome['activation_function'] == 'sigmoid':
            self.activation = nn.Sigmoid()
        elif genome['activation_function'] == 'tanh':
            self.activation = nn.Tanh()

    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.fc2(x)
        return x

class Organism:
    def __init__(self):

        self.genome = {'batch_size': np.random.choice(batch_sizes),'activation_function': np.random.choice(activation_functions)}

        self.brain = NeuralNet(28*28, 10, self.genome)

        self.fitness = 0
        self.age = 0
        self.mutation_rate = 0.01

        self.learning_rate = 1e-4

        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = torch.optim.Adam(self.brain.parameters(), lr=self.learning_rate)

    def train(self, train_dataset, num_epochs):

        self.brain = NeuralNet(28*28, 10, self.genome)
        self.optimizer = torch.optim.Adam(self.brain.parameters(), lr=self.learning_rate)

        self.num_epochs = num_epochs
        self.train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=int(self.genome['batch_size']), shuffle=True)
        self.brain.train()

        self.f1_log = []

        for epoch in range(self.num_epochs):
            running_loss = 0.0
            self.y_true = []
            self.y_pred = []
            for inputs, targets in self.train_loader:
                self.optimizer.zero_grad()
                outputs = self.brain(inputs)
                self.outputs = outputs
                predicted = torch.argmax(outputs,1)
                loss = self.criterion(outputs, targets)
                loss.backward()
                self.optimizer.step()
                running_loss += loss.item()
                self.y_true.append(targets.cpu().numpy())
                self.y_pred.append(predicted.cpu().numpy())

            self.y_pred = [item for sublist in self.y_pred for item in sublist]
            self.y_true = [item for sublist in self.y_true for item in sublist]
            f1 = f1_score(self.y_true, self.y_pred, average='macro')

            average_loss = running_loss / len(train_dataset)
            self.f1_log.append(f1)

    def evaluate_fitness(self, val_dataset):
        self.brain.eval()
        y_true = []
        y_pred = []
        for inputs, targets in val_dataset:
            outputs = self.brain(inputs)
            predicted = torch.argmax(outputs)
            y_true.append(targets.cpu().numpy())
            y_pred.append(predicted.cpu().numpy())

        f1 = f1_score(y_true, y_pred, average='macro')
        return f1

    def mutate(self):
        for i in range(len(self.genome)):
            if random.random() < self.mutation_rate:
                self.genome['batch_size'] = np.random.choice(batch_sizes)
            if random.random() < self.mutation_rate:
                self.genome['activation_function'] = np.random.choice(activation_functions)

pop_size = 10
num_parents = 4
num_gen = 100

def one_point_crossover(parent1, parent2):
    child1, child2 =  copy.copy(parent1), copy.copy(parent2)
    crossover_point = random.randint(1, len(parent1.brain.fc1.weight) - 1)
    child1.brain.fc1.weight.data[:crossover_point], child2.brain.fc1.weight.data[:crossover_point] = \
        parent2.brain.fc1.weight.data[:crossover_point], parent1.brain.fc1.weight.data[:crossover_point]
    return child1, child2

population = [Organism() for _ in range(pop_size)]
max_fitness = []
avg_fitness = []

for generation in range(num_gen):
    fitness_scores = []
    for organism in population:
        organism.train(train_dataset, 1)
        fitness_scores.append(organism.evaluate_fitness(val_dataset))


    total_fitness = sum(fitness_scores)
    selection_probabilities = [organism / total_fitness for organism in fitness_scores]
    selected_indices = random.choices(range(pop_size), weights=selection_probabilities, k=pop_size)

    new_population = []
    for _ in range(0, pop_size, 2):
        parent1 = population[selected_indices.pop()]
        parent2 = population[selected_indices.pop()]

        child1, child2 = one_point_crossover(parent1, parent2)

        child1.mutate()
        child2.mutate()

        new_population.extend([child1, child2])

    print(f"Generation {generation}: Best Fitness - {max(fitness_scores)}")
    max_fitness.append(max(fitness_scores))
    avg_fitness.append(np.mean(fitness_scores))
    population = new_population

print(f"Best Genome: {population[np.argmax(fitness_scores)].genome}")

champion = population[np.argmax(fitness_scores)]
champion.brain = NeuralNet(28*28, 10, champion.genome)
champion.optimizer = torch.optim.Adam(champion.brain.parameters(), lr=champion.learning_rate)

champion.train(train_dataset,50)
champion.evaluate_fitness(test_dataset)

plt.figure(figsize=(10, 6))
plt.plot(champion.f1_log, marker='o')

plt.title(f'Champion F1 Log: {champion.genome}')
plt.xlabel('Epoch')
plt.ylabel('F1 Score')

plt.savefig('genetic_f1_log.png', dpi=300, format='png', bbox_inches='tight')

plt.show()

plt.figure(figsize=(10, 6))
plt.plot(max_fitness, marker='o', color='green', label='Max Fitness')
plt.plot(avg_fitness, marker='.', color='red', label='Avg. Fitness')

plt.title(f'Max and Avereage Fitness Scores per Generation')
plt.xlabel('Generation')
plt.ylabel('F1 Score')
plt.legend()
plt.savefig('genetic_fitness.png', dpi=300, format='png', bbox_inches='tight')

plt.show()